{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practical Use Case : Valeo\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Valeo_Logo.svg/2560px-Valeo_Logo.svg.png\" alt=\"Logo\" width=\"200\"/>\n",
        "\n",
        "### Nom : AMROUN\n",
        "### PrÃ©nom : Abdelkader\n",
        "### Mail : aek.amroun@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "#### Imports and installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "6d48a098-db23-46d9-fd52-f91fbdc37ec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (2.12.0)\n",
            "Requirement already satisfied: transformers in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (4.32.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: six in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/abdelkader/anaconda3/envs/MLA11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch datasets transformers flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import collections\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    default_data_collator\n",
        ")\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "<!-- # 3.Fine-tuning a model on a question-answering task -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BlpZKRbz7bt"
      },
      "source": [
        "<!-- In this notebook, we will see how to fine-tune the **DistillBert** model to a question answering task, which is the task of extracting the answer to a question from a given context. \n",
        "\n",
        "as for chosen dataset it is squad v1 ... add talk about it\n",
        "We will use the `Trainer` API to fine-tune a model on it.\n",
        "\n",
        "\n",
        "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text. -->\n",
        "\n",
        "\n",
        "# 3. Fine-Tuning DistillBERT for Question Answering\n",
        "\n",
        "In this notebook, we will explore how to fine-tune the **DistillBERT** model for a **Question Answering (QA)** task. Specifically, the goal of QA is to extract a precise answer from a given context based on a posed question. Rather than generating new text, our model will focus on selecting the most relevant substring from the context as the answer.\n",
        "\n",
        "For this task, we will use the **SQuAD v1 (Stanford Question Answering Dataset)**, one of the most widely used datasets for QA. SQuAD v1 consists of over 100,000 question-answer pairs, where the answer is always a contiguous span of text from the context, making it an ideal dataset for extractive question answering.\n",
        "\n",
        "To streamline the fine-tuning process, we will leverage Hugging Face's powerful `Trainer` API, which simplifies model training and evaluation. By the end of this notebook, we will have a fine-tuned DistillBERT model capable of efficiently answering questions by extracting the appropriate text from a given passage.\n",
        "\n",
        "**Note:** The model will not generate new text but rather focus on identifying the correct answer span within the provided context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "<!-- This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly: -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Checkpoint\n",
        "\n",
        "In this notebook, we use the pre-trained **DistilBERT** model with the checkpoint `\"distilbert-base-uncased\"`. DistilBERT is a compressed version of BERT that retains 97% of BERT's performance while being 60% faster and smaller, making it an ideal choice for resource-efficient fine-tuning. \n",
        "\n",
        "Since we are working with **SQuAD v1**, which only contains answerable questions, we set `squad_v2 = False`. This indicates that we are not using the SQuAD v2 dataset, which includes unanswerable questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "\n",
        "squad_v2 = False\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "We use the `datasets` library from Hugging Face to easily load and handle our dataset. Depending on whether we are working with **SQuAD v1** or **SQuAD v2**, the dataset is loaded accordingly:\n",
        "\n",
        "- If `squad_v2 = True`, the **SQuAD v2** dataset is loaded, which includes unanswerable questions.\n",
        "- Since we've set `squad_v2 = False` for this task, we are loading the **SQuAD v1** dataset, which contains only answerable questions.\n",
        "\n",
        "The code simplifies dataset management by automatically selecting the appropriate version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "7aebc1a6-b239-4c44-a377-19df4f36381d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWiVUF0jIrIv",
        "outputId": "99eba26a-54f7-4277-8f9c-3c92195ff89d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et9nW_NDz7bu"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, we need to select a split, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HrpprwIrIz",
        "outputId": "bb7aec7f-9eb1-4d17-a5c9-3ca380825115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1S4RY3hz7bu"
      },
      "source": [
        "We can see the answers are indicated by their start position in the text (here at character 515) and their full text, which is a substring of the context as we mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "### Displaying Random Dataset Elements\n",
        "\n",
        "To better understand the dataset, we use a helper function `show_random_elements` to display random examples from it. The function selects a specified number of random entries (by default 10) and presents them in a clear table format.\n",
        "\n",
        "Hereâ€™s how the function works:\n",
        "- It ensures the number of requested examples does not exceed the size of the dataset.\n",
        "- It selects random, non-repeating indices from the dataset.\n",
        "- The data is then converted into a **pandas DataFrame** for easy viewing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "def show_random_elements(dataset, num_examples=10):\n",
        "    # Ensure we don't attempt to sample more elements than are available in the dataset.\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "\n",
        "    # Randomly pick 'num_examples' unique indices from the dataset.\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        # Ensure we don't pick the same index more than once.\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    # Convert the selected entries into a pandas DataFrame for easy display.\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "\n",
        "    # Transform ClassLabel features into human-readable names for better understanding.\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "\n",
        "    # Display the DataFrame in HTML format for better visualization.\n",
        "    display(HTML(df.to_html()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "SZy5tRB_IrI7",
        "outputId": "4b6577eb-51fe-4df0-cc6c-dc6059d8af05",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56f87773a6d7ea1400e1769a</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>The city has undergone many changes to its governance over the centuries and once again became administratively independent from Hampshire County as it was made into a unitary authority in a local government reorganisation on 1 April 1997, a result of the 1992 Local Government Act. The district remains part of the Hampshire ceremonial county.</td>\n",
              "      <td>What ceremonial county does Southampton still belong to?</td>\n",
              "      <td>{'text': ['Hampshire'], 'answer_start': [316]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5728d1664b864d1900164eb7</td>\n",
              "      <td>Asthma</td>\n",
              "      <td>For those with severe persistent asthma not controlled by inhaled corticosteroids and LABAs, bronchial thermoplasty may be an option. It involves the delivery of controlled thermal energy to the airway wall during a series of bronchoscopies. While it may increase exacerbation frequency in the first few months it appears to decrease the subsequent rate. Effects beyond one year are unknown. Evidence suggests that sublingual immunotherapy in those with both allergic rhinitis and asthma improve outcomes.</td>\n",
              "      <td>What treatment helps improve those with allergic rhinitis and asthma?</td>\n",
              "      <td>{'text': ['sublingual immunotherapy'], 'answer_start': [415]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570cef0dfed7b91900d45b09</td>\n",
              "      <td>Digestion</td>\n",
              "      <td>Teeth (singular tooth) are small whitish structures found in the jaws (or mouths) of many vertebrates that are used to tear, scrape, milk and chew food. Teeth are not made of bone, but rather of tissues of varying density and hardness, such as enamel, dentine and cementum. Human teeth have a blood and nerve supply which enables proprioception. This is the ability of sensation when chewing, for example if we were to bite into something too hard for our teeth, such as a chipped plate mixed in food, our teeth send a message to our brain and we realise that it cannot be chewed, so we stop trying.</td>\n",
              "      <td>What happens when you bite something you cant chew?</td>\n",
              "      <td>{'text': ['our teeth send a message to our brain and we realise that it cannot be chewed, so we stop trying.'], 'answer_start': [502]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_random_elements(datasets[\"train\"], num_examples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a HuggingFace Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXNLu_-nIrJI",
        "outputId": "d16fd991-bcd1-4044-c045-2d8e7d00b4d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the HuggingFace Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3Z1OBm8rz7bv"
      },
      "outputs": [],
      "source": [
        "\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "We can directly call this tokenizer on two sentences (the first one is the answer, and the second one is for the context):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hBlsrHIrJL",
        "outputId": "ddd2fa45-e472-468f-8b46-8a3ac241608e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Maxime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utaqk0YBz7bw"
      },
      "source": [
        "In question answering, handling very long documents presents a unique challenge. For other tasks, we usually truncate inputs that exceed the model's maximum sentence length. However, in QA, truncating part of the context could remove the answer we're trying to extract. To address this, we split a long document into multiple smaller chunks (input features), each within the model's maximum length (or the length defined by a hyper-parameter). Additionally, to avoid cutting off answers that might span across two chunks, we introduce some overlap between the chunks. This overlap is controlled by a hyper-parameter `doc_stride`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J0OcG0muz7bw"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we never want to truncate the question, only the context, else the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and by passing the stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxg87Hjfz7bz"
      },
      "source": [
        "For this notebook to work with any kind of models, we need to account for the special case where the model expects padding on the left (in which case we switch the order of the question and the context):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_1Wnl9tz7bz"
      },
      "source": [
        "### Tokenizing and Preparing Features for Training\n",
        "\n",
        "In this section, we prepare our training data by tokenizing the input examples and handling long contexts that might get split into multiple chunks. Here's how the `prepare_train_features` function works:\n",
        "\n",
        "- **Whitespace Cleanup**: We remove unnecessary left-side whitespace from the questions to ensure efficient tokenization.\n",
        "  \n",
        "- **Tokenization with Overflows**: The function tokenizes the question and context, truncating as needed. If a context is too long, it is split into multiple features using a stride, which creates overlapping chunks. This helps prevent losing the answer if it's near the split point between two chunks.\n",
        "\n",
        "- **Mapping Features to Original Examples**: Since one example can generate multiple features due to splitting, we use the `overflow_to_sample_mapping` to keep track of which feature corresponds to which original example.\n",
        "\n",
        "- **Offset Mapping**: The `offset_mapping` tells us where each token aligns with the original character position in the context, helping us accurately label the start and end positions of the answers.\n",
        "\n",
        "- **Labeling the Start and End Positions**: For each tokenized feature, we identify where the answer starts and ends. If the answer is not found in the current span, we mark the CLS token as the answer. If the answer is found, we adjust the token indices to match the character positions of the answer.\n",
        "\n",
        "This process ensures that our tokenized examples are ready for training, with accurate labels for start and end positions of the answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CTv5uVdQz7bz"
      },
      "outputs": [],
      "source": [
        "# Check if the tokenizer pads on the right side (for the question) or the left (for the context).\n",
        "pad_on_right = tokenizer.padding_side == \"right\"\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    # Remove unnecessary whitespace from the beginning of the questions.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize the questions and contexts with truncation and padding.\n",
        "    # If the context is too long, split it into overlapping chunks (controlled by doc_stride).\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],   # Tokenize question if padding is on the right.\n",
        "        examples[\"context\" if pad_on_right else \"question\"],   # Otherwise, tokenize context first.\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",  # Truncate only the context or question.\n",
        "        max_length=max_length,   # Set the max length for each chunk.\n",
        "        stride=doc_stride,       # Use a stride to create overlapping chunks for long contexts.\n",
        "        return_overflowing_tokens=True,   # Keep track of overflowing tokens (for multiple features per example).\n",
        "        return_offsets_mapping=True,      # Return the character position mapping for each token.\n",
        "        padding=\"max_length\",    # Ensure all tokens are padded to the same max length.\n",
        "    )\n",
        "\n",
        "    # Create a mapping between features (splits) and their corresponding original examples.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # Offset mappings help us align tokens with their original character positions in the context.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Initialize lists for storing start and end positions of the answers.\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    # Iterate over each feature's offsets to determine the answer positions.\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # Index of the CLS token, used for labeling unanswerable questions.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Get the sequence IDs (0 for question, 1 for context) to help locate the context in the feature.\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Map the current feature back to its original example.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If the example has no answer, mark it as the CLS token (unanswerable).\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Get the start and end character positions of the answer in the original context.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Find the token start index corresponding to the answer's start character.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # Find the token end index corresponding to the answer's end character.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # If the answer is outside the current span, label it with the CLS token.\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Adjust the token start and end indices to exactly match the answer span.\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "### Creating a Smaller Subset and Tokenizing the Dataset\n",
        "\n",
        "In this section, we create a smaller subset of the training and evaluation datasets for faster experimentation and debugging. This allows us to fine-tune our model without using the full dataset initially. Here's the process:\n",
        "\n",
        "- **Subset Creation**: We select a smaller portion of the training dataset (first 5008 examples) and the validation dataset (first 160 examples). This helps reduce training time during experimentation considering my training setup (Tesla T4 on Google Colab)\n",
        "\n",
        "- **Tokenization**: We apply the `prepare_train_features` function to both subsets to tokenize the examples. During this process, the original columns are removed, and only the necessary features (e.g., input IDs, attention masks, token type IDs, start and end positions) are kept.\n",
        "\n",
        "This approach is useful for running faster iterations while fine-tuning the model on a smaller portion of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "# Create a smaller subset of the dataset for faster experimentation and debugging.\n",
        "# Here, we select the first 5008 examples from the training set and the first 160 examples from the validation set.\n",
        "small_train_dataset = datasets[\"train\"].select(range(5008))\n",
        "small_eval_dataset = datasets[\"validation\"].select(range(160))\n",
        "\n",
        "# Apply the prepare_train_features function to tokenize the training subset.\n",
        "# We use the map function to tokenize the dataset in batches.\n",
        "# The original columns are removed to keep only the necessary tokenized features.\n",
        "tokenized_train_dataset = small_train_dataset.map(\n",
        "    prepare_train_features,  # Function used to tokenize and process the data.\n",
        "    batched=True,            # Apply the function to batches of examples.\n",
        "    remove_columns=small_train_dataset.column_names  # Remove the original columns (context, question, etc.).\n",
        ")\n",
        "\n",
        "# Apply the same tokenization process to the evaluation subset.\n",
        "tokenized_eval_dataset = small_eval_dataset.map(\n",
        "    prepare_train_features,\n",
        "    batched=True,\n",
        "    remove_columns=small_eval_dataset.column_names\n",
        ")\n",
        "\n",
        "# If you want to use the full dataset for training, the following commented line can be used to tokenize the entire dataset.\n",
        "# tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready for training, we can download the pretrained model and fine-tune it. Since our task is question answering, we use the `AutoModelForQuestionAnswering` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlqNaB8jIrJW",
        "outputId": "8a35db6f-ead5-4343-a4d0-5b178d1393d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "### Setting Up Training Arguments\n",
        "\n",
        "We define the training configuration using the `TrainingArguments` class from Hugging Face's `transformers` library. These arguments control how the model is fine-tuned and evaluated. Here's a breakdown of the key settings:\n",
        "\n",
        "- **Model Name**: We extract the base model name from the `model_checkpoint` and use it for naming the output directory.\n",
        "  \n",
        "- **Batch Size**: We set a batch size of 16 for both training and evaluation, which is the number of examples processed at once on each device.\n",
        "\n",
        "- **Learning Rate**: The learning rate is set to `2e-5`, a common value for fine-tuning transformer models, controlling how fast the model's weights are updated.\n",
        "\n",
        "- **Evaluation Strategy**: The model will be evaluated at the end of each epoch, allowing us to monitor its performance.\n",
        "\n",
        "- **Training Epochs**: The model will be trained for 3 full passes through the dataset (epochs).\n",
        "\n",
        "- **Weight Decay**: A value of 0.01 is used for weight decay, which helps regularize the model and prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bliy8zgjIrJY",
        "outputId": "fb96c816-1649-4da7-fd73-ff51f070dfb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Extract the model's base name from the model checkpoint path.\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "# Define the batch size for training and evaluation. 16 examples will be processed at a time per device.\n",
        "batch_size = 16\n",
        "\n",
        "# Set up the training arguments using Hugging Face's TrainingArguments class.\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}_squad\",  # The output directory where the model and checkpoints will be saved.\n",
        "    evaluation_strategy=\"epoch\",      # Evaluate the model at the end of every epoch.\n",
        "    learning_rate=2e-5,               # Set the learning rate, controlling how fast the model learns.\n",
        "    per_device_train_batch_size=batch_size,  # Number of examples per batch during training.\n",
        "    per_device_eval_batch_size=batch_size,   # Number of examples per batch during evaluation.\n",
        "    num_train_epochs=3,               # Number of training epochs (full passes through the dataset).\n",
        "    weight_decay=0.01,                # Apply weight decay (regularization) to prevent overfitting.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-squad\"` or `\"huggingface/bert-finetuned-squad\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKsf-fpez7b0"
      },
      "source": [
        "Then we will need a data collator that will batch our processed examples together, here the default one will work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Wp7p_SWBz7b0"
      },
      "outputs": [],
      "source": [
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "We will evaluate our model and compute metrics in the next section (this is a very long operation, so we will only compute the evaluation loss during training).\n",
        "\n",
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "# Initialize the Trainer with the specified parameters\n",
        "trainer = Trainer(\n",
        "    model,  # The model to be trained\n",
        "    args,  # Training arguments\n",
        "    # train_dataset=tokenized_datasets[\"train\"],  # Original training dataset (commented out)\n",
        "    # eval_dataset=tokenized_datasets[\"validation\"],  # Original evaluation dataset (commented out)\n",
        "    train_dataset=tokenized_train_dataset,  # Tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,  # Tokenized evaluation dataset\n",
        "    data_collator=data_collator,  # Data collator for dynamic padding\n",
        "    tokenizer=tokenizer,  # Tokenizer used for preprocessing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uNx5pyRlIrJh",
        "outputId": "5087f18c-4abd-4c24-e857-25f01a80a143"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='957' max='957' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [957/957 09:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.173265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.806700</td>\n",
              "      <td>1.675167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.806700</td>\n",
              "      <td>1.651030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=957, training_loss=2.1279300701655566, metrics={'train_runtime': 563.223, 'train_samples_per_second': 27.176, 'train_steps_per_second': 1.699, 'total_flos': 1499832261817344.0, 'train_loss': 2.1279300701655566, 'epoch': 3.0})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UubRqQjRz7b1"
      },
      "source": [
        "Since this training is particularly long, let's save the model just in case we need to restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZBEyCc1Zz7b1"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPBM8lrNz7b1"
      },
      "source": [
        "# 4. Model Evaluation (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhzZUq9wz7b1"
      },
      "source": [
        "To evaluate our fine-tuned DistilBERT model for the QA task, we need to map the model's predictions back to specific parts of the context. The model outputs logits for both the start and end positions of the predicted answers. Let's take a batch from our validation dataloader and examine the model's output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LVPAwR7z7b2",
        "outputId": "f0f90e9e-b495-4c7e-ebd8-43d0e091a276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'start_logits', 'end_logits'])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a batch from the evaluation dataloader.\n",
        "# The loop stops after retrieving the first batch.\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "# Move the batch data to the device (GPU or CPU) being used by the model.\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "\n",
        "# Disable gradient calculations since we're in evaluation mode (no need for backpropagation).\n",
        "with torch.no_grad():\n",
        "    # Pass the batch through the model and get the output.\n",
        "    output = trainer.model(**batch)\n",
        "\n",
        "# Display the keys of the model's output (these typically include logits, start/end positions for QA tasks).\n",
        "output.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xURluS0Zz7b2"
      },
      "source": [
        "The output of the model is a dict-like object that contains the loss (since we provided labels), the start and end logits. We won't need the loss for our predictions, let's have a look a the logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3yN54iWz7b2",
        "outputId": "ed7e9f41-e965-4a4b-e287-465806e22f16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 384]), torch.Size([16, 384]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1YU20W1z7b2"
      },
      "source": [
        "We have one logit for each feature and each token. The most obvious thing to predict an answer for each feature is to take the index for the maximum of the start logits as a start position and the index of the maximum of the end logits as an end position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui2hFYLuz7b2",
        "outputId": "f695e3be-7870-4b7a-83e7-4d098fb578fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 46,  57,  89,  43, 167, 162,  72, 160, 162, 159,  73,  41,  80,  91,\n",
              "         156,  35], device='cuda:0'),\n",
              " tensor([ 47,  47,  92,  44, 141, 150,  75, 148, 150, 147,  76,  42,  83,  94,\n",
              "         158,  35], device='cuda:0'))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8pUXPgfz7b2"
      },
      "source": [
        "While this approach works well in many cases, there are situations where the predicted answer might be invalid. For instance, the start position might be greater than the end position, or the model might highlight a span of text within the question itself instead of the answer. In such cases, itâ€™s useful to consider the second-best prediction and check whether it provides a valid answer instead.\n",
        "\n",
        "However, selecting the second-best answer isn't straightforward. Should we choose the second-best start position with the best end position, or vice versa? And if the second-best prediction is also invalid, determining the third-best becomes even more complex.\n",
        "\n",
        "To resolve this, we will classify our answers based on a score derived by summing the start and end logits. Instead of exhaustively ordering all possible answers, we will use a hyperparameter, `n_best_size`, to limit the number of candidates. We'll select the top indices from the start and end logits, generate the corresponding answers, and check their validity. Once validated, we sort them by their score and select the best option. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1hAI_FZDz7b2"
      },
      "outputs": [],
      "source": [
        "n_best_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9kTTB3Kz7b3"
      },
      "source": [
        "Once we have our `valid_answers`, we can sort them by their `score` and retain only the best one. The remaining challenge is ensuring that a predicted span is part of the context (and not the question) and extracting the corresponding text. To achieve this, we need to add two crucial elements to our validation features:\n",
        "\n",
        "- The ID of the example that generated the feature, as each example can produce multiple features (as explained earlier).\n",
        "- The offset mapping, which provides a mapping from token indices to character positions within the context.\n",
        "\n",
        "To accommodate these needs, we will reprocess the validation set using a function that is slightly modified from `prepare_train_features`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DJgZLV1Fz7b3"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some questions may have unnecessary whitespace at the beginning, which isn't useful and can interfere with\n",
        "    # truncation of the context (by using up token space). So we remove the leading whitespace from the questions.\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize the examples, ensuring truncation is applied where necessary and overflows are handled with a stride.\n",
        "    # If a context is too long, it may produce several features, each with some overlapping context from the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],  # Tokenize either the question or context first based on the padding direction\n",
        "        examples[\"context\" if pad_on_right else \"question\"],  # Tokenize the other part (context or question)\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",  # Truncate the second (or first) sequence based on padding direction\n",
        "        max_length=max_length,  # Maximum token length allowed\n",
        "        stride=doc_stride,  # Stride determines how much overlap between chunks when handling long contexts\n",
        "        return_overflowing_tokens=True,  # Return any overflowed tokens that resulted from the tokenization\n",
        "        return_offsets_mapping=True,  # Return the offset mapping to track positions of tokens relative to the original text\n",
        "        padding=\"max_length\",  # Pad sequences to the max length\n",
        "    )\n",
        "\n",
        "    # Since one example might produce several features due to a long context, we need a way to map each feature\n",
        "    # back to its original example. \"overflow_to_sample_mapping\" provides this mapping.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We create a list to store the IDs of the examples that generated each feature. We also store offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    # Iterate through all tokenized features (input_ids) to process each feature individually.\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Get the sequence IDs for the current feature, which helps distinguish between question and context.\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # Determine the index for the context part of the input based on whether we're padding on the right.\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # Map the current feature to its original example using the sample_mapping.\n",
        "        sample_index = sample_mapping[i]\n",
        "        # Store the ID of the example that produced this feature.\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # For each token's offset mapping, set it to None if it belongs to the question (i.e., not part of the context).\n",
        "        # This helps later in determining which tokens belong to the context.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)  # Keep only context offsets, nullify others\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRn8cQl2z7b3"
      },
      "source": [
        "And like in training, we can apply that function to our validation set easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eZlBe0xDz7b3"
      },
      "outputs": [],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pdU8WOIz7b3"
      },
      "source": [
        "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uEdIj_eyz7b3",
        "outputId": "e8f991d5-30f7-4002-fbcc-172c68bba733"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoq8GY6mz7b3"
      },
      "source": [
        "The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AgqhzHUoz7b4"
      },
      "outputs": [],
      "source": [
        "# Set the format of the validation features to match the current format type.\n",
        "# We specify the columns that we want to include, which are all the keys in the validation features' schema (features).\n",
        "validation_features.set_format(\n",
        "    type=validation_features.format[\"type\"],  # Maintain the same format type (e.g., 'torch', 'numpy', etc.)\n",
        "    columns=list(validation_features.features.keys())  # Set the columns to include all available feature keys\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l__SPQ7Qz7b4"
      },
      "source": [
        "We can now refine the test we had before: since we set `None` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tblJY97Iz7b4"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcVSB60Iz7b5"
      },
      "source": [
        "The `postprocess_qa_predictions` function handles post-processing of raw predictions from our question-answering model. The process involves the following key steps:\n",
        "\n",
        "1. **Mapping examples to features**: Since multiple features can correspond to a single example, a map is created to track which features belong to which example.\n",
        "\n",
        "2. **Logits and score calculation**: The function retrieves the start and end logits for each feature and calculates scores for potential answer spans.\n",
        "\n",
        "3. **Handling null predictions**: For models like SQuAD v2, where no-answer predictions are valid, the function tracks the null prediction score (based on the `[CLS]` token).\n",
        "\n",
        "4. **Filtering and validation**: The function filters out invalid answers (e.g., answers where the end index is before the start index or spans that are too long). It uses the `offset_mapping` to map token positions back to the original text.\n",
        "\n",
        "5. **Selecting the best answer**: For each example, the valid answers are sorted by score, and the highest-scoring answer is selected. If the null prediction score is higher than the best answer score (in SQuAD v2), the model outputs no answer.\n",
        "\n",
        "The function returns the final predictions for each example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "49o4cvpKz7b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zJevoMcz7b5"
      },
      "source": [
        "And we can apply our post-processing function to our raw predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c5641c3494094949b2666e4a6b697383",
            "99f8d274c4324462967ec8f4fb282a76",
            "668638337fe1409a8056d953fe2c8496",
            "51009c4c04464d47ad504397e4e70ca5",
            "b32c5075f1994f4c93761fd39f92276e",
            "fdcb6e305d6b4e25be160aafb49c45dd",
            "2bf7238efa844053b7e7b369dc36c6da",
            "8b0e6df70ebf4833bd0a0d99ae64d558",
            "bcd2cfa970af4d929215be7a99529b0c",
            "69bb942567d346e19500b3e75360be4f",
            "65c476d4c7f7440487503fbd65ab05cb"
          ]
        },
        "id": "M3eL2f7sz7b5",
        "outputId": "07553a68-7215-4ff6-f409-9a8e7eb9d1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing 10570 example predictions split into 10784 features.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5641c3494094949b2666e4a6b697383",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Call the postprocessing function to convert the raw model predictions into final, human-readable answers.\n",
        "# This function takes the validation examples, tokenized features, and the raw start and end logits from the model.\n",
        "\n",
        "final_predictions = postprocess_qa_predictions(\n",
        "    datasets[\"validation\"],      # The original validation dataset with examples and contexts.\n",
        "    validation_features,         # The processed validation features including tokenized inputs and offset mappings.\n",
        "    raw_predictions.predictions  # The raw start and end logits predicted by the model for each feature.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we load the `squad` metric from the datasets library, which is specifically designed for evaluating models on the **SQuAD (Stanford Question Answering Dataset)**. This metric provides two main scores:\n",
        "\n",
        "1. **Exact Match (EM)**: This measures the percentage of predictions that match exactly with the ground truth answer. It requires the predicted answer to be identical to the correct answer, including punctuation and word choice.\n",
        "   \n",
        "2. **F1 Score**: This score considers the overlap between the predicted answer and the ground truth in terms of words. The F1 score is a balance between precision (the proportion of predicted words that are relevant) and recall (the proportion of relevant words that are predicted).\n",
        "\n",
        "### How to interpret the metrics:\n",
        "- **Exact Match (EM)**: A high EM score indicates that the model is providing answers that are highly accurate in terms of matching the reference answers exactly. A perfect score of 100% means every answer is correct without any differences.\n",
        "- **F1 Score**: The F1 score allows for partial credit when the model's predicted answer overlaps with the correct answer. It is useful when an exact match is not found, but the model captures most of the relevant information. A higher F1 score (close to 100%) means better performance.\n",
        "\n",
        "Both metrics are commonly used in QA tasks to assess the model's ability to understand and generate accurate answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How SQuAD Metrics Are Calculated\n",
        "\n",
        "In the context of the SQuAD (Stanford Question Answering Dataset), the two key evaluation metricsâ€”Exact Match (EM) and F1 Scoreâ€”are calculated as follows:\n",
        "\n",
        "#### 1. Exact Match (EM)\n",
        "\n",
        "The Exact Match metric checks if the predicted answer exactly matches the ground truth answer (the correct answer from the dataset).\n",
        "\n",
        "###### Step-by-step Calculation:\n",
        "\n",
        "1. The prediction is compared to the ground truth after normalizing both. Normalization includes lowercasing, removing articles (a, an, the), punctuation, and extra whitespace.\n",
        "2. If the normalized predicted answer is exactly the same as the normalized ground truth answer, the score for that prediction is 1.\n",
        "3. If not, the score is 0.\n",
        "4. The Exact Match score for the entire dataset is the average of the EM scores across all examples in the dataset.\n",
        "\n",
        "##### Formula:\n",
        "\n",
        "$$ \\text{Exact Match (EM)} = \\frac{\\text{Number of exact matches}}{\\text{Total number of questions}} \\times 100 $$\n",
        "\n",
        "##### Example:\n",
        "\n",
        "- **Ground truth:** \"New York City\"\n",
        "  \n",
        "  **Prediction:** \"New York City\"\n",
        "  \n",
        "  **EM = 1** (Exact Match)\n",
        "\n",
        "- **Ground truth:** \"New York City\"\n",
        "  \n",
        "  **Prediction:** \"NYC\"\n",
        "  \n",
        "  **EM = 0** (Not an Exact Match)\n",
        "\n",
        "#### 2. F1 Score\n",
        "\n",
        "The F1 Score is a more forgiving metric that measures the word-level overlap between the predicted answer and the ground truth. It is calculated using both precision and recall.\n",
        "\n",
        "##### Precision:\n",
        "\n",
        "The fraction of predicted words that are in the ground truth answer.\n",
        "\n",
        "$$ \\text{Precision} = \\frac{\\text{Number of overlapping words}}{\\text{Total number of predicted words}} $$\n",
        "\n",
        "##### Recall:\n",
        "\n",
        "The fraction of ground truth words that are in the predicted answer.\n",
        "\n",
        "$$ \\text{Recall} = \\frac{\\text{Number of overlapping words}}{\\text{Total number of ground truth words}} $$\n",
        "\n",
        "##### F1 Score:\n",
        "\n",
        "The harmonic mean of precision and recall, giving a score between 0 and 1. A perfect F1 score of 1 indicates a perfect match, and 0 indicates no overlap.\n",
        "\n",
        "$$ \\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
        "\n",
        "##### Step-by-step Calculation:\n",
        "\n",
        "1. Break the predicted answer and the ground truth into words.\n",
        "2. Count how many words overlap.\n",
        "3. Calculate precision and recall based on the overlap.\n",
        "4. Compute the F1 score from precision and recall.\n",
        "\n",
        "##### Example:\n",
        "\n",
        "- **Ground truth:** \"New York City\"\n",
        "  \n",
        "  **Prediction:** \"New York\"\n",
        "  \n",
        "  **Overlap:** \"New York\" (2 words overlap)\n",
        "  \n",
        "  $$ \\text{Precision} = \\frac{2}{2} = 1.0 $$\n",
        "  \n",
        "  $$ \\text{Recall} = \\frac{2}{3} \\approx 0.67 $$\n",
        "  \n",
        "  $$ \\text{F1} = \\frac{2 \\times 1.0 \\times 0.67}{1.0 + 0.67} \\approx 0.80 $$\n",
        "\n",
        "- **Ground truth:** \"New York City\"\n",
        "  \n",
        "  **Prediction:** \"Los Angeles\"\n",
        "  \n",
        "  **Overlap:** None\n",
        "  \n",
        "  $$ \\text{Precision} = \\frac{0}{2} = 0 $$\n",
        "  \n",
        "  $$ \\text{Recall} = \\frac{0}{3} = 0 $$\n",
        "  \n",
        "  $$ \\text{F1} = 0 $$\n",
        "\n",
        "##### Key Points:\n",
        "\n",
        "- EM is stricter: It demands an exact word match between the predicted answer and the correct one.\n",
        "- F1 is more flexible: It accounts for partial matches, making it a useful measure for cases where the prediction contains part of the correct answer but isn't a perfect match.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH3e7ar_z7b5",
        "outputId": "d91fc446-e5e6-4eab-9545-17d35147ca85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-69495f532713>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n"
          ]
        }
      ],
      "source": [
        "metric = load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7OlsF0iz7b5"
      },
      "source": [
        "Then we can call compute on it. We just need to format predictions and labels a bit as it expects a list of dictionaries and not one big dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz6u-IGGz7b5",
        "outputId": "ee2f5f87-ede4-4656-8099-ff98ef86c498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 54.34247871333964, 'f1': 64.99205071723347}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a list of dictionaries where each dictionary contains an \"id\" and its corresponding \"prediction_text\".\n",
        "# This is generated by iterating over the key-value pairs (k, v) of the 'final_predictions' dictionary.\n",
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "\n",
        "# Create a list of dictionaries where each dictionary contains an \"id\" and its corresponding \"answers\".\n",
        "# This is generated by iterating over the \"validation\" dataset and extracting the \"id\" and \"answers\" for each example (ex).\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
        "\n",
        "# Compute the metric by comparing the formatted predictions against the references.\n",
        "# 'predictions' is the list of formatted predictions created earlier, and 'references' is the ground truth.\n",
        "metric.compute(predictions=formatted_predictions, references=references)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmibj4Mmz7b5"
      },
      "source": [
        "### Comment on Results\n",
        "\n",
        "The results from the fine-tuning of the DistilBERT model on the SQuAD dataset, achieving an **Exact Match (EM) of 54.34%** and an **F1 score of 64.99%**, are lower than what would typically be expected from models trained on the full dataset. However, this outcome was anticipated due to the limited training data and computational resources used.\n",
        "\n",
        "- **Training Data**: The model was trained on less than 10% of the SQuAD dataset, which significantly restricts the amount of information the model can learn from. Given the complexity of the dataset, which requires a deep understanding of context to answer questions accurately, the reduced dataset likely resulted in the model not fully capturing all nuances necessary for higher accuracy.\n",
        "\n",
        "- **Training Setup**: The training was conducted on Google Colab using a **Tesla T4 GPU**, and to manage the available computational power, the model was fine-tuned for only **3 epochs**. These restrictions meant that the model had limited opportunities to optimize and learn the patterns in the data. With more computational resources, such as a more powerful GPU or the ability to train on the full dataset for a longer period, the model's performance would likely improve.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "These scores are reflective of the expected limitations given the constraints of training on a small fraction of the dataset and the available computational resources. Despite the lower scores, the results demonstrate that the model is learning and generalizing to some extent, especially when considering the significant limitations imposed by the setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "breakpoint() # To stop the execution since the following code serves as an explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzU5b2-pFVvs"
      },
      "source": [
        "# 5. API Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Creation for Question Answering with Flask\n",
        "\n",
        "In this implementation, I created a simple API using **Flask** to serve a fine-tuned **DistilBERT** model for Question Answering. The following components are covered:\n",
        "\n",
        "1. **Model Loading**: \n",
        "    - The model and tokenizer were loaded from a locally saved directory (`distillbert_squad`) using the `AutoModelForQuestionAnswering` and `DistilBertTokenizerFast` classes from the HuggingFace `transformers` library.\n",
        "\n",
        "2. **Pipeline Setup**: \n",
        "    - A HuggingFace `pipeline` was configured to handle the Question Answering task, which takes input as a question and a context and returns an answer from the context.\n",
        "\n",
        "3. **API Endpoint**:\n",
        "    - A `/predict` endpoint was defined using the Flask `@app.route` decorator, which listens for **POST** requests. \n",
        "    - The request body must include two fields: a \"questiomn\" and a \"context\". \n",
        "    - The input is validated, and if either field is missing, an error response with a 400 status code is returned.\n",
        "\n",
        "4. **Inference**:\n",
        "    - The model generates a prediction (answer) and confidence score using the `qa_pipeline`. This output is then returned to the client as a JSON object, which includes the original question, the context, the predicted answer, and the modelâ€™s confidence score.\n",
        "\n",
        "5. **Deployment**:\n",
        "    - The Flask app is set to run on `0.0.0.0` to allow external access, using port 5000. For local testing with ngrok, `run_with_ngrok` can be uncommented.\n",
        "\n",
        "This API allows easy interaction with the fine-tuned DistilBERT model for question answering tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "908ObImFFa69",
        "outputId": "a495ccd2-932e-41f4-9a14-b0c9b0071dea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-09-05 17:04:55.170598: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-05 17:04:55.548217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-05 17:04:55.548455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-05 17:04:55.592171: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-05 17:04:55.697270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-05 17:04:57.023056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.20.10.8:5000\n",
            "Press CTRL+C to quit\n",
            "127.0.0.1 - - [05/Sep/2024 17:05:10] \"GET / HTTP/1.1\" 404 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:05:10] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
            "172.20.10.8 - - [05/Sep/2024 17:05:24] \"GET / HTTP/1.1\" 404 -\n",
            "172.20.10.8 - - [05/Sep/2024 17:05:24] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:07:29] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:07:52] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:08:44] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:11:47] \"POST /predict HTTP/1.1\" 400 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:13:32] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:15:05] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:16:12] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:16:42] \"POST /predict HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [05/Sep/2024 17:17:01] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from transformers import DistilBertTokenizerFast, pipeline, AutoModelForQuestionAnswering\n",
        "# from flask_ngrok import run_with_ngrok  # Uncomment if using ngrok for local deployment\n",
        "# from optimum.onnxruntime import ORTModelForQuestionAnswering  # Optional: For ONNX runtime\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the fine-tuned DistilBERT model and tokenizer from the specified directory\n",
        "model_dir = \"distillbert_squad\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_dir)  # Load the model for Question Answering\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)    # Load the corresponding tokenizer\n",
        "\n",
        "# Set up a HuggingFace pipeline for Question Answering using the loaded model and tokenizer\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define an API endpoint at /predict that accepts POST requests\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Parse the JSON request body for the question and context\n",
        "    data = request.get_json(force=True)\n",
        "\n",
        "    # Extract the question and context from the request\n",
        "    question = data.get(\"question\", \"\")  # Default to empty string if question is not provided\n",
        "    context = data.get(\"context\", \"\")    # Default to empty string if context is not provided\n",
        "\n",
        "    # Validate the input to ensure both question and context are present\n",
        "    if not question or not context:\n",
        "        return jsonify({\"error\": \"Please provide both question and context\"}), 400  # Return error if missing\n",
        "\n",
        "    # Use the QA pipeline to generate the answer based on the input question and context\n",
        "    result = qa_pipeline({\n",
        "        \"question\": question,\n",
        "        \"context\": context\n",
        "    })\n",
        "\n",
        "    # Return the result as a JSON response, including the question, context, answer, and confidence score\n",
        "    return jsonify({\n",
        "        \"question\": question,\n",
        "        \"context\": context,\n",
        "        \"answer\": result[\"answer\"],  # Extract the predicted answer\n",
        "        \"score\": result[\"score\"]     # Extract the confidence score of the prediction\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Optional: For running the app with ngrok if testing locally\n",
        "    # run_with_ngrok(app)\n",
        "\n",
        "    # Start the Flask app on host 0.0.0.0 (all network interfaces) and port 5000\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### API Testing with `unittest`\n",
        "\n",
        "To ensure the API functions correctly, I wrote unit tests using Python's built-in `unittest` framework. The following describes the test cases implemented:\n",
        "\n",
        "1. **Setup**: \n",
        "   - In the `setUp()` method, I initialized the Flask app's test client using `app.test_client()`. This allows us to simulate HTTP requests to the API without running the server.\n",
        "   - Testing mode is enabled by setting `self.app.testing = True`, which helps catch any errors during test execution.\n",
        "\n",
        "2. **Test Cases**:\n",
        "   - **`test_predict_success()`**: \n",
        "     - This test checks if the `/predict` endpoint works as expected when valid input (both question and context) is provided.\n",
        "     - A POST request is sent with a sample question and context related to \"quantization\".\n",
        "     - The test verifies:\n",
        "       - The response format is JSON.\n",
        "       - The status code is `200 OK`.\n",
        "       - The response contains an `answer` key, and the confidence `score` is greater than a reasonable threshold (0.2).\n",
        "   \n",
        "   - **`test_predict_missing_data()`**:\n",
        "     - This test checks the API's behavior when the input is incomplete (missing context).\n",
        "     - A POST request is sent with only a question, but no context.\n",
        "     - The test verifies:\n",
        "       - The API returns a `400 Bad Request` status due to missing data.\n",
        "       - The response contains an `error` key indicating the issue.\n",
        "\n",
        "3. **Running the Tests**:\n",
        "   - The tests are executed using `unittest.main()` when the script is run directly. This will automatically discover and run all test cases in the class.\n",
        "\n",
        "These tests ensure the core functionality of the API and handle common input validation scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unittest\n",
        "import json\n",
        "from flask_app import app  # Import the Flask app from the main API module\n",
        "\n",
        "# Define a test case class using unittest framework to test the Flask API\n",
        "class FlaskAPITestCase(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Set up a test client for the Flask app\n",
        "        self.app = app.test_client()  # Flask provides a test client to simulate API requests\n",
        "        self.app.testing = True  # Enable testing mode for Flask (disables error catching)\n",
        "\n",
        "    def test_predict_success(self):\n",
        "        # Test case to check the API response when both question and context are provided\n",
        "\n",
        "        # Sample payload with a valid question and context\n",
        "        payload = {\n",
        "            \"question\": \"What is quantization?\",\n",
        "            \"context\": \"Quantization is a technique to reduce model size and speed up inference.\"\n",
        "        }\n",
        "\n",
        "        # Send a POST request to the /predict endpoint with the payload as JSON\n",
        "        response = self.app.post('/predict', data=json.dumps(payload), content_type='application/json')\n",
        "\n",
        "        # Ensure the response is in JSON format\n",
        "        self.assertEqual(response.content_type, 'application/json')\n",
        "\n",
        "        # Ensure the status code is 200 (OK)\n",
        "        self.assertEqual(response.status_code, 200)\n",
        "\n",
        "        # Parse the response data from JSON\n",
        "        data = json.loads(response.data)\n",
        "\n",
        "        # Verify that the 'answer' key exists in the response\n",
        "        self.assertIn(\"answer\", data)\n",
        "\n",
        "        # Ensure the confidence score ('score') is greater than a reasonable threshold (0.2 in this case)\n",
        "        self.assertGreater(data[\"score\"], 0.1)\n",
        "\n",
        "    def test_predict_missing_data(self):\n",
        "        # Test case to handle scenarios where either the question or context is missing\n",
        "\n",
        "        # Payload with a missing context field\n",
        "        payload = {\n",
        "            \"question\": \"What is quantization?\"\n",
        "        }\n",
        "\n",
        "        # Send a POST request with the incomplete payload (missing context)\n",
        "        response = self.app.post('/predict', data=json.dumps(payload), content_type='application/json')\n",
        "\n",
        "        # Check that the response status is 400 (Bad Request) due to missing data\n",
        "        self.assertEqual(response.status_code, 400)\n",
        "\n",
        "        # Parse the response data\n",
        "        data = json.loads(response.data)\n",
        "\n",
        "        # Check that the error message is returned in the response\n",
        "        self.assertIn(\"error\", data)\n",
        "\n",
        "# If this script is run directly, execute the unit tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bf7238efa844053b7e7b369dc36c6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51009c4c04464d47ad504397e4e70ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69bb942567d346e19500b3e75360be4f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_65c476d4c7f7440487503fbd65ab05cb",
            "value": "â€‡10570/10570â€‡[00:31&lt;00:00,â€‡356.30it/s]"
          }
        },
        "65c476d4c7f7440487503fbd65ab05cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "668638337fe1409a8056d953fe2c8496": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b0e6df70ebf4833bd0a0d99ae64d558",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcd2cfa970af4d929215be7a99529b0c",
            "value": 10570
          }
        },
        "69bb942567d346e19500b3e75360be4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0e6df70ebf4833bd0a0d99ae64d558": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99f8d274c4324462967ec8f4fb282a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdcb6e305d6b4e25be160aafb49c45dd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2bf7238efa844053b7e7b369dc36c6da",
            "value": "100%"
          }
        },
        "b32c5075f1994f4c93761fd39f92276e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd2cfa970af4d929215be7a99529b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5641c3494094949b2666e4a6b697383": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99f8d274c4324462967ec8f4fb282a76",
              "IPY_MODEL_668638337fe1409a8056d953fe2c8496",
              "IPY_MODEL_51009c4c04464d47ad504397e4e70ca5"
            ],
            "layout": "IPY_MODEL_b32c5075f1994f4c93761fd39f92276e"
          }
        },
        "fdcb6e305d6b4e25be160aafb49c45dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
